{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "flush-astrology",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Benson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "packed-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Datasets\n",
    "df=pd.read_csv('datasets/true_fake_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "embedded-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop([\"title\", \"date\",\"label\",\"combined\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rolled-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "grave-queens",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace = True)\n",
    "df.drop([\"index\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "allied-depth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text      2\n",
       "target    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bound-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mexican-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub(\"\\\\W\",\" \",text) \n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "upper-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "outdoor-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "accessory-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"text\"]\n",
    "y = df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "executed-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-coordination",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "virtual-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe = Pipeline([('count_vectorization', CountVectorizer()),\n",
    "                 ('tfidf_vectorization', TfidfTransformer()),\n",
    "                 ('LR', LogisticRegression())])\n",
    "\n",
    "LR = pipe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ordinary-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dt = LR.predict(x_test)\n",
    "\n",
    "LR.score(x_test, y_test)\n",
    "#saving this model to the disk\n",
    "model_file = 'models/logistic_regression.sav'\n",
    "pickle.dump(LR,open(model_file,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "parallel-mining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      6199\n",
      "           1       0.97      0.96      0.97      5583\n",
      "\n",
      "    accuracy                           0.97     11782\n",
      "   macro avg       0.97      0.97      0.97     11782\n",
      "weighted avg       0.97      0.97      0.97     11782\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-proportion",
   "metadata": {},
   "source": [
    "# Decision Tree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "graphic-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipe = Pipeline([('count_vectorization', CountVectorizer()),\n",
    "                 ('tfidf_vectorization', TfidfTransformer()),\n",
    "                 ('DT', DecisionTreeClassifier())])\n",
    "\n",
    "DT = pipe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "married-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dt = DT.predict(x_test)\n",
    "\n",
    "DT.score(x_test, y_test)\n",
    "#saving this model to the disk\n",
    "model_file = 'models/decision_tree.sav'\n",
    "pickle.dump(DT,open(model_file,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "normal-syria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94      6199\n",
      "           1       0.94      0.93      0.94      5583\n",
      "\n",
      "    accuracy                           0.94     11782\n",
      "   macro avg       0.94      0.94      0.94     11782\n",
      "weighted avg       0.94      0.94      0.94     11782\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-cowboy",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "nasty-highway",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline([('count_vectorization', CountVectorizer()),\n",
    "                 ('tfidf_vectorization', TfidfTransformer()),\n",
    "                 ('RFC', RandomForestClassifier())])\n",
    "\n",
    "RFC = pipe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "elder-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rfc = RFC.predict(x_test)\n",
    "\n",
    "RFC.score(x_test, y_test)\n",
    "#saving this model to the disk\n",
    "model_file = 'models/random_forest.sav'\n",
    "pickle.dump(RFC,open(model_file,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "buried-developer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      6199\n",
      "           1       0.97      0.97      0.97      5583\n",
      "\n",
      "    accuracy                           0.97     11782\n",
      "   macro avg       0.97      0.97      0.97     11782\n",
      "weighted avg       0.97      0.97      0.97     11782\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_rfc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-source",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "rocky-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_label(n):\n",
    "    if n == 0:\n",
    "        return \"Fake News\"\n",
    "    elif n == 1:\n",
    "        return \"True News\"\n",
    "    \n",
    "def detecting_fake_news(news_text):\n",
    "    testing_news = {\"text\":[news_text]}\n",
    "    new_def_test = pd.DataFrame(testing_news)\n",
    "    \n",
    "    new_def_test[\"text\"] = new_def_test[\"text\"].apply(preprocess) \n",
    "    new_x_test = new_def_test[\"text\"]\n",
    "    \n",
    "    pred_LR = LR.predict(new_x_test)\n",
    "    pred_DT = DT.predict(new_x_test)\n",
    "    pred_RFC = RFC.predict(new_x_test)\n",
    "    \n",
    "\n",
    "    return print(\"\\n\\nLR Prediction: {} \\nDT Prediction: {} \\nRFC Prediction: {} \".format(prediction_label(pred_LR[0]),                                                                                                      \n",
    "                                                                                                              prediction_label(pred_DT[0]), \n",
    "                                                                                                              prediction_label(pred_RFC[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "running-denmark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dwayne Johnson defending a bunch of beleaguered millennials sounds like our kind of movie, but we’ll have to make do with the Instagram video The Rock posted Friday decrying a recent interview published by The Daily Star in which the actor dragged the “snowflake generation.” Griped Johnson in the article, “So many good people fought for freedom and equality — but this generation are looking for a reason to be offended.” The thing is, according to the actor, he actually loves millennials (in addition to everyone else) and that interview is entirely made up.“The interview never took place. Never happened. Never said any of those words,” Johnson said on Instagram, saying he was “baffled” to learn he was reportedly in a heated battle with people born between 1982 and 2004. “Completely untrue. One hundred percent fabricated.” Adds the actor, “I always encourage empathy, I encourage growth but most importantly, I encourage everybody to be exactly who they want to be.” And really, in the end, aren’t we all snowflakes compared to The Rock?\n",
      "\n",
      "\n",
      "LR Prediction: Fake News \n",
      "DT Prediction: True News \n",
      "RFC Prediction: Fake News \n"
     ]
    }
   ],
   "source": [
    "news_text = str(input())\n",
    "detecting_fake_news(news_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-brick",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
